{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMV0OXrsPnYI7ezNMzREN4N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Ensemble Learning"],"metadata":{"id":"V9rjTfxPNtp2"}},{"cell_type":"markdown","source":["Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n","\n","- Ensemble Learning is a machine learning technique where multiple individual models, often called base estimators or weak learners, are trained and then combined to solve a particular computational intelligence problem.\n","\n","- The key idea behind ensemble learning is to leverage the \"wisdom of the crowd\" principle. By aggregating the predictions of several diverse models, the ensemble model achieves better performance and generalization than any single constituent model. The collective decision tends to be more robust and accurate because the errors or biases of individual models often cancel each other out."],"metadata":{"id":"2rc2X20fNvTd"}},{"cell_type":"markdown","source":["Question 2: What is the difference between Bagging and Boosting?\n","- Models are trained independently and in parallelModels are trained sequentially and adaptively.\n","- Data Sampling - Uses bootstrap samples (random sampling with replacement) to create different training sets for each modelUses the full dataset where subsequent models focus on the samples that were misclassified or poorly predicted by previous models.\n","- Model Weights - All individual models are typically given equal weight in the final predictionModels are weighted, with better-performing models (later models that correct errors) often having a greater influence.\n","- Primary GoalDecrease variance and prevent overfitting, especially for complex or high-variance base models like Decision TreesDecrease bias by converting a set of weak learners into a strong learner.\n","- ExamplesRandom Forest, Bagging with Decision Trees, Bagging with k-Nearest NeighborsAdaBoost, Gradient Boosting Machines (GBM), XGBoost, LightGBM."],"metadata":{"id":"eLgWN5xGN4sG"}},{"cell_type":"markdown","source":["Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n","- Bootstrap sampling is a resampling technique where a subset of data is randomly drawn from the original dataset with replacement16. This means that any one data point can be selected multiple times in a single bootstrap sample, or not at all17. The resulting bootstrap sample has the same size as the original dataset but is a unique variation of it.\n","- Role in Bagging (e.g., Random Forest):\n","- Bootstrap sampling is fundamental to Bagging (Bootstrap Aggregating)19. Its role is to:Introduce Diversity: By creating $N$ distinct training datasets (where $N$ is the number of base estimators), it ensures that the individual models (e.g., Decision Trees in a Random Forest) are trained on slightly different perspectives of the data20.\n","- Ensure Independence: Training models on these independent bootstrap samples introduces randomness and helps decorrelate the individual models' errors21.\n","- Reduce Variance: When these diverse models are aggregated (e.g., by averaging their predictions or taking a majority vote), the high variance of individual models (like deep Decision Trees) is significantly reduced, leading to a more robust and generalized final ensemble."],"metadata":{"id":"4WFH6DNnOZii"}},{"cell_type":"markdown","source":["Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n","- Out-of-Bag (OOB) samples are the data points from the original training set that were not included in the bootstrap sample used to train a particular base estimator (e.g., a single Decision Tree in a Random Forest). Because bootstrap sampling is \"with replacement,\" each base estimator in a Bagging process typically uses about 63.2% of the original data for training, leaving the remaining $\\approx$36.8% as OOB samples for that specific estimator.\n","- Use of OOB Score for Evaluation:\n","- The OOB score is a method for evaluating the generalization performance of an ensemble model without the need for a separate validation set or explicit cross-validation.\n","- Prediction: For each data point in the original training set, only the base estimators that did not use that data point in their training (i.e., the OOB models for that point) are used to make a prediction.\n","- Aggregation: The OOB predictions from all relevant base estimators for a given data point are aggregated (e.g., majority vote for classification, averaging for regression)\n","- Score Calculation: The OOB score (e.g., accuracy, $R^2$) is calculated by comparing these aggregated OOB predictions with the true target values across the entire training set.\n","- The OOB score is considered an unbiased estimate of the model's test set performance, effectively acting as an internal cross-validation."],"metadata":{"id":"MkqbErxjO8qU"}},{"cell_type":"markdown","source":["Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n","- Feature importance analysis differs significantly between a single Decision Tree and a Random Forest. In a single Decision Tree, feature importance is calculated based on how much a feature reduces impurity (like Gini impurity or entropy) when used for a split within that specific tree. This method is highly unstable and susceptible to noise, as a minor change in the training data can drastically alter the tree structure and, consequently, the feature ranking. Furthermore, a deep single tree can have importance scores biased toward features that performed well on its particular training subset, leading to potential overfitting. In contrast, a Random Forest provides a more stable and reliable estimate of feature importance. The importance score for a feature in a Random Forest is the average of its importance scores across all the individual Decision Trees in the ensemble. The use of bootstrap sampling and random feature subsetting for each tree decorrelates the individual models , and the averaging process mitigates overfitting and smooths out the variance , offering a more global and robust view of feature relevance across the entire dataset."],"metadata":{"id":"ixc0kDa3N2aj"}},{"cell_type":"code","source":["''' Question 6: Write a Python program to:\n","\n","- Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n","\n","- Train a Random Forest Classifier\n","\n","Print the top 5 most important features based on feature importance scores. '''\n","\n","import pandas as pd\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# 1. Load the dataset\n","data = load_breast_cancer()\n","X = pd.DataFrame(data.data, columns=data.feature_names)\n","y = data.target\n","\n","# 2. Train a Random Forest Classifier\n","# Using a fixed random state for reproducibility\n","model = RandomForestClassifier(n_estimators=100, random_state=42)\n","model.fit(X, y)\n","\n","# 3. Get feature importance scores\n","feature_importances = pd.Series(model.feature_importances_, index=X.columns)\n","\n","# Sort the features and get the top 5\n","top_5_features = feature_importances.nlargest(5)\n","\n","# Print the top 5 most important features\n","print(\"Top 5 Most Important Features:\")\n","print(top_5_features)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rFj1YjfZP_XQ","executionInfo":{"status":"ok","timestamp":1762098548907,"user_tz":-330,"elapsed":3600,"user":{"displayName":"Aman Jat","userId":"03874188547701461901"}},"outputId":"bc634ba7-d8ac-4142-dc56-fdb361233cf0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Top 5 Most Important Features:\n","worst area              0.139357\n","worst concave points    0.132225\n","mean concave points     0.107046\n","worst radius            0.082848\n","worst perimeter         0.080850\n","dtype: float64\n"]}]},{"cell_type":"code","source":["''' Question 7: Write a Python program to:\n","\n","- Train a Bagging Classifier using Decision Trees on the Iris dataset\n","\n","- Evaluate its accuracy and compare with a single Decision Tree '''\n","\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# 1. Load the Iris dataset\n","data = load_iris()\n","X, y = data.data, data.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42\n",")\n","\n","# 2. Single Decision Tree\n","dt_model = DecisionTreeClassifier(random_state = 42)\n","dt_model.fit(X_train, y_train)\n","dt_pred = dt_model.predict(X_test)\n","dt_accuracy = accuracy_score(y_test, dt_pred)\n","\n","# Bagging Classifier Using Decision Trees\n","bagging_model = BaggingClassifier(estimator = DecisionTreeClassifier(random_state= 42),n_estimators = 100, random_state = 42, n_jobs = 1)\n","bagging_model.fit(X_train, y_train)\n","bagging_pred = bagging_model.predict(X_test)\n","bagging_accuracy = accuracy_score(y_test, bagging_pred)\n","\n","# --- 4. Compare Accuracies ---\n","print(f\"Accuracy of Single Decision Tree: {dt_accuracy:.4f}\")\n","print(f\"Accuracy of Bagging Classifier: {bagging_accuracy:.4f}\")\n","print(\"\\nComparison: The Bagging Classifier improves accuracy by combining multiple independent Decision Trees, reducing variance.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5GkwHJ3QiOA","executionInfo":{"status":"ok","timestamp":1762099114954,"user_tz":-330,"elapsed":416,"user":{"displayName":"Aman Jat","userId":"03874188547701461901"}},"outputId":"d72be8d7-46b7-465d-f979-bbd3cc500f5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of Single Decision Tree: 1.0000\n","Accuracy of Bagging Classifier: 1.0000\n","\n","Comparison: The Bagging Classifier improves accuracy by combining multiple independent Decision Trees, reducing variance.\n"]}]},{"cell_type":"code","source":["''' Question 8: Write a Python program to:\n","\n","- Train a Random Forest Classifier\n","\n","- Tune hyperparameters max_depth and n_estimators using GridSearchCV\n","\n","- Print the best parameters and final accuracy '''\n","\n","import pandas as pd\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# 1. Load the dataset (using Breast Cancer for robustness)\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42\n",")\n","\n","# 2. Define the model and the parameter grid\n","rf = RandomForestClassifier(random_state=42)\n","\n","# Define the hyperparameters to tune\n","param_grid = {\n","    'n_estimators': [50, 100, 200], # Number of trees in the forest\n","    'max_depth': [5, 10, None]       # Maximum depth of the tree (None = full depth)\n","}\n","\n","# 3. Tune hyperparameters using GridSearchCV\n","# Using 5-fold cross-validation\n","grid_search = GridSearchCV(\n","    estimator=rf,\n","    param_grid=param_grid,\n","    cv=5,\n","    scoring='accuracy',\n","    n_jobs=-1,\n","    verbose=0\n",")\n","grid_search.fit(X_train, y_train)\n","\n","# 4. Print the best parameters and final accuracy\n","best_params = grid_search.best_params_\n","best_model = grid_search.best_estimator_\n","\n","# Evaluate on the test set\n","final_pred = best_model.predict(X_test)\n","final_accuracy = accuracy_score(y_test, final_pred)\n","\n","print(f\"Best Parameters found by GridSearchCV: {best_params}\")\n","print(f\"Test Accuracy with Best Model: {final_accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u6j8dSzbR8Hf","executionInfo":{"status":"ok","timestamp":1762099340472,"user_tz":-330,"elapsed":13868,"user":{"displayName":"Aman Jat","userId":"03874188547701461901"}},"outputId":"3afa97cc-be45-4790-dba4-efbdafa0eadc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Parameters found by GridSearchCV: {'max_depth': 10, 'n_estimators': 200}\n","Test Accuracy with Best Model: 0.9708\n"]}]},{"cell_type":"code","source":["''' Question 9: Write a Python program to:\n","\n","- Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n","\n","- Compare their Mean Squared Errors (MSE) '''\n","\n","import numpy as np\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n","from sklearn.metrics import mean_squared_error\n","\n","# 1. Load the California Housing dataset\n","data = fetch_california_housing()\n","X, y = data.data, data.target\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42\n",")\n","\n","# --- 2. Train Bagging Regressor ---\n","# Base estimator is a Decision Tree Regressor\n","bagging_reg = BaggingRegressor(\n","    estimator=DecisionTreeRegressor(random_state=42),\n","    n_estimators=50,\n","    random_state=42,\n","    n_jobs=-1\n",")\n","bagging_reg.fit(X_train, y_train)\n","bagging_pred = bagging_reg.predict(X_test)\n","bagging_mse = mean_squared_error(y_test, bagging_pred)\n","\n","# --- 3. Train Random Forest Regressor ---\n","# Random Forest is essentially a specialized Bagging of Decision Trees\n","rf_reg = RandomForestRegressor(\n","    n_estimators=50,\n","    random_state=42,\n","    n_jobs=-1\n",")\n","rf_reg.fit(X_train, y_train)\n","rf_pred = rf_reg.predict(X_test)\n","rf_mse = mean_squared_error(y_test, rf_pred)\n","\n","# --- 4. Compare MSEs ---\n","print(f\"Mean Squared Error (MSE) for Bagging Regressor: {bagging_mse:.4f}\")\n","print(f\"Mean Squared Error (MSE) for Random Forest Regressor: {rf_mse:.4f}\")\n","print(\"\\nComparison: Random Forest often performs better due to its additional feature-randomness component, which further reduces correlation between trees compared to standard Bagging.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iWMOgEhyTi--","executionInfo":{"status":"ok","timestamp":1762099494749,"user_tz":-330,"elapsed":16906,"user":{"displayName":"Aman Jat","userId":"03874188547701461901"}},"outputId":"6f173554-7ded-4768-c0e2-4bbb24e1928b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Squared Error (MSE) for Bagging Regressor: 0.2579\n","Mean Squared Error (MSE) for Random Forest Regressor: 0.2577\n","\n","Comparison: Random Forest often performs better due to its additional feature-randomness component, which further reduces correlation between trees compared to standard Bagging.\n"]}]},{"cell_type":"markdown","source":["- Question 10: You are working as a data scientist at a financial institution to predict loan\n","default. You have access to customer demographic and transaction history data.\n","You decide to use ensemble techniques to increase model performance.\n","Explain your step-by-step approach to:\n","● Choose between Bagging or Boosting\n","● Handle overfitting\n","● Select base models\n","● Evaluate performance using cross-validation\n","● Justify how ensemble learning improves decision-making in this real-world\n","context.\n","\n","- Step 1: Choose Ensemble Technique\n","- Choice: Boosting (e.g., XGBoost, LightGBM).\n","- Justification: Loan default prediction is a bias-sensitive problem, as missing a defaulter (False Negative) is costly. Boosting excels at reducing bias by training models sequentially to focus on correcting the misclassified samples of prior models, generally yielding higher predictive accuracy than Bagging.\n","\n","- Step 2: Select Base Models\n","- Selection: Shallow Decision Trees.\n","- Justification: Decision Trees are the standard base learners for Boosting. Using shallow trees (e.g., max_depth $\\in [3, 7]$) ensures they remain \"weak learners,\" which are essential for the sequential, additive process of boosting.\n","\n","- Step 3: Handle Overfitting (Regularization)\n","- Techniques :\n","- Shrinkage (Learning Rate): Use a small learning rate (e.g., $\\eta < 0.1$) to make the model learn gradually and act as regularization.\n","- Subsampling: Apply row subsampling (stochastic gradient boosting) and column subsampling (feature randomness) to inject noise and decorrelate the trees, similar to a Random Forest.\n","- Early Stopping: Monitor the model's performance on a held-out validation set and stop training if the metric (e.g., AUC-ROC) doesn't improve after a set number of rounds.\n","\n","- Step 4: Evaluate Performance using Cross-Validation\n","- Metric: Area Under the ROC Curve (AUC-ROC). This metric is robust to the high class imbalance (few defaulters) common in financial data.\n","- Method: Stratified k-Fold Cross-Validation. This ensures that the percentage of defaulters is maintained across every fold, providing a reliable and stable estimate of the model's true generalization performance.\n","\n","-   Step 5: Justify Improvement in Decision-Making\n","Ensemble learning, through Boosting, improves decision-making by:\n","- Reducing Financial Risk: Provides significantly higher accuracy (AUC-ROC), resulting in fewer missed defaulters (False Negatives), which directly reduces institutional loss.\n","- Providing Risk Insights: Feature importance scores identify the strongest risk factors (e.g., specific debt ratios), enabling the institution to create targeted risk mitigation and policy adjustments.\n","- Optimizing Resource Allocation: The reliable high-performance model ensures manual review and intervention resources are focused only on applicants with the highest predicted risk."],"metadata":{"id":"YWjU2d_7UpBy"}},{"cell_type":"code","source":["''' Question 10. Python Code '''\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import StratifiedKFold, GridSearchCV\n","from xgboost import XGBClassifier\n","from sklearn.metrics import roc_auc_score\n","\n","# --- 1. Create a Synthetic Financial Dataset (Simulating loan default) ---\n","# Highly imbalanced and complex, which is typical for default prediction\n","X, y = make_classification(\n","    n_samples=5000,\n","    n_features=20,\n","    n_informative=10,\n","    n_redundant=5,\n","    n_classes=2,\n","    n_clusters_per_class=1,\n","    weights=[0.9, 0.1],  # Simulating 10% default rate (imbalanced)\n","    flip_y=0.02,\n","    random_state=42\n",")\n","\n","# --- 2. Train and Tune Boosting Classifier (XGBoost) ---\n","xgb_model = XGBClassifier(\n","    objective='binary:logistic',\n","    use_label_encoder=False,\n","    eval_metric='logloss',\n","    random_state=42\n",")\n","\n","# Define a simplified grid for demonstration of Hyperparameter Tuning\n","param_grid = {\n","    'n_estimators': [100, 200],\n","    'max_depth': [3, 5],\n","    'learning_rate': [0.05, 0.1]\n","}\n","\n","# Use Stratified K-Fold for imbalanced data\n","cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# GridSearchCV for tuning and evaluation\n","grid_search = GridSearchCV(\n","    estimator=xgb_model,\n","    param_grid=param_grid,\n","    scoring='roc_auc',  # Using AUC-ROC, the preferred metric\n","    cv=cv,\n","    n_jobs=-1,\n","    verbose=0\n",")\n","grid_search.fit(X, y)\n","\n","# --- 3. Print Best Parameters and Performance ---\n","best_params = grid_search.best_params_\n","best_model = grid_search.best_estimator_\n","\n","# Get Cross-Validation Score (Mean AUC-ROC)\n","cv_score = grid_search.cv_results_['mean_test_score'][grid_search.best_index_]\n","\n","print(f\"--- Boosting (XGBoost) Model Performance ---\")\n","print(f\"Best Parameters found: {best_params}\")\n","print(f\"Mean 5-Fold Stratified CV AUC-ROC Score: {cv_score:.4f}\")\n","\n","# --- 4. Feature Importance Analysis (for decision justification) ---\n","# Note: Feature numbering starts from 0 for the synthetic data\n","feature_importances = pd.Series(\n","    best_model.feature_importances_,\n","    index=[f'Feature_{i}' for i in range(X.shape[1])]\n",")\n","top_5_features = feature_importances.nlargest(5)\n","print(\"\\nTop 5 Most Important Features (Risk Factors):\")\n","print(top_5_features)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"949mu9gUWMF-","executionInfo":{"status":"ok","timestamp":1762100110997,"user_tz":-330,"elapsed":16191,"user":{"displayName":"Aman Jat","userId":"03874188547701461901"}},"outputId":"f01e6e05-8d55-455a-f173-a4834c7df13c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [16:15:10] WARNING: /workspace/src/learner.cc:790: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["--- Boosting (XGBoost) Model Performance ---\n","Best Parameters found: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n","Mean 5-Fold Stratified CV AUC-ROC Score: 0.9504\n","\n","Top 5 Most Important Features (Risk Factors):\n","Feature_1     0.209698\n","Feature_7     0.156602\n","Feature_17    0.067861\n","Feature_12    0.067256\n","Feature_2     0.056079\n","dtype: float32\n"]}]}]}